{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import simple_driving\n",
    "#import pybullet_envs\n",
    "import pybullet as p\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import random\n",
    "\n",
    "#display = Display(visible=0, size=(400, 300))\n",
    "#display.start()\n",
    "\n",
    "def display_video(frames, framerate=30):\n",
    "  \"\"\"Generates video from `frames`.\n",
    "\n",
    "  Args:\n",
    "    frames (ndarray): Array of shape (n_frames, height, width, 3).\n",
    "    framerate (int): Frame rate in units of Hz.\n",
    "\n",
    "  Returns:\n",
    "    Display object.\n",
    "  \"\"\"\n",
    "  height, width, _ = frames[0].shape\n",
    "  dpi = 70\n",
    "  orig_backend = matplotlib.get_backend()\n",
    "  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "  matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "  ax.set_axis_off()\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_position([0, 0, 1, 1])\n",
    "  im = ax.imshow(frames[0])\n",
    "  def update(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "  interval = 1000/framerate\n",
    "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                  interval=interval, blit=True, repeat=False)\n",
    "  return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(env, state, episodes, episode, model):\n",
    "    \"\"\"Selects an action to take based on a uniformly random sampled number. \n",
    "    If this number is greater than epsilon then returns action with the largest\n",
    "    Q-value at the current state. Otherwise it returns a random action. This\n",
    "    version decays epsilon from a large number to a small number over the duration\n",
    "    of training. This results in highly likely random actions at the start and\n",
    "    eventually biasing actions towards those with high q-values towards the end.\n",
    "\n",
    "    Args:\n",
    "        env: gym object.\n",
    "        state: current state\n",
    "        episodes: maximum number of episodes\n",
    "        episode: number of episodes played so far\n",
    "        model: Q-function approximator\n",
    "\n",
    "    Returns:\n",
    "        Action to be executed for next step.\n",
    "    \"\"\"\n",
    "    EPS_START = 0.99\n",
    "    EPS_END = 0.15\n",
    "    EPS_DECAY = episodes\n",
    "    sample = np.random.uniform(0, 1)\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * episode / EPS_DECAY)\n",
    "    if sample > eps_threshold:\n",
    "        q_values = model(torch.tensor([state], dtype=torch.float32))    # predict q-values for state\n",
    "        return q_values.argmax().item()                                 # return action with highest q-value\n",
    "    else:\n",
    "        return np.random.choice(np.array(range(8)))  # incorporate prior here to prevent flapping too much during exploration (agent will always keep trying to fly into the sky otherwise)\n",
    "        #return np.random.choice([6,7,8])\n",
    "\n",
    "def simulate(env, max_episode_length, episodes, episode, model):\n",
    "    \"\"\"Rolls out an episode of actions to be used for learning.\n",
    "\n",
    "    Args:\n",
    "        env: gym object.\n",
    "        episodes: maximum number of episodes\n",
    "        episode: number of episodes played so far\n",
    "\n",
    "    Returns:\n",
    "        Dataset of episodes for training the RL agent containing states, actions and rewards.\n",
    "    \"\"\"\n",
    "    D = []\n",
    "    state = env.reset()                                                   # line 2\n",
    "    done = False\n",
    "    env.getExtendedObservation()\n",
    "    prev_reward = 0\n",
    "    #while True:                                                             # here I chose to not let episode end but you could replace with the line below\n",
    "    for step in range(max_episode_length):                                  # line 3\n",
    "        action = epsilon_greedy(env, state, episodes, episode, model)  # line 4\n",
    "        next_state, reward, done,info = env.step(action)                   # line 5\n",
    "\n",
    "        D.append([state, action, reward, next_state])                       # line 7\n",
    "        state = next_state                                                  # line 8\n",
    "        if done:                                                            # if we fall into\n",
    "            break\n",
    "    return D                                                                # line 10\n",
    "\n",
    "def approx_q_learning(env, gamma, episodes, max_episode_length, model, loss_fn, optimizer):\n",
    "    \"\"\"Main loop of Approximate Q-learning algorithm.\n",
    "\n",
    "    Args:\n",
    "        env: gym object.\n",
    "        gamma: discount factor - determines how much to value future actions\n",
    "        episodes: number of episodes to play out\n",
    "        max_episode_length: maximum number of steps for episode roll out\n",
    "        model: Q-function approximator\n",
    "        loss_fn: the loss function for our function approximator\n",
    "        optimizer: for backpropagating the gradient of our loss\n",
    "\n",
    "    Returns:\n",
    "        Q-function which is used to derive policy.\n",
    "    \"\"\"\n",
    "    total_reward = 0\n",
    "    for episode in range(episodes):                                             # slightly different to line 3, we just run until maximum episodes played out\n",
    "        D = simulate(env, max_episode_length, episodes, episode, model)    # line 4\n",
    "        q_values_batch = []\n",
    "        target_batch = []\n",
    "        for data in D:                                                          # data = [state, action, reward, next_state]  (line 5)\n",
    "            ####################### update Q values (line 6-7) #########################\n",
    "            state  = data[0]\n",
    "            action = data[1]\n",
    "            reward = data[2]\n",
    "            #print(f\"this is reward = {reward}\")\n",
    "            next_state = data[3]\n",
    "            q_values = model( torch.tensor([state], dtype=torch.float32) )            # predict Q-value for current state\n",
    "            q_values_next = model(torch.tensor([next_state], dtype=torch.float32))  # predict Q-value for next state\n",
    "            target = q_values.clone().detach()\n",
    "            target[0][action] = reward + gamma * q_values_next.max().item()         # loss between prediction and true Q-value we just found from interacting with the env\n",
    "            total_reward += data[2]\n",
    "            loss = loss_fn(q_values, target)                                        # compute loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()                                                         # backpropogate the loss (compute gradients)\n",
    "            optimizer.step()                                                        # update model using gradients\n",
    "            #########################################################################    \n",
    "        if episode % 100 == 0:\n",
    "            print(\"average total reward per episode batch since episode \", episode, \": \", total_reward/ float(100))\n",
    "            total_reward = 0\n",
    "    return model  # line 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\mrham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\mrham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\mrham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\mrham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "c:\\Users\\mrham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:137: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'list'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\mrham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\spaces\\box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  logger.warn(\"Casting input x to numpy array.\")\n",
      "c:\\Users\\mrham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "C:\\Users\\mrham\\AppData\\Local\\Temp\\ipykernel_14720\\3201796454.py:86: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  q_values = model( torch.tensor([state], dtype=torch.float32) )            # predict Q-value for current state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average total reward per episode batch since episode  0 :  0.6348607445165831\n",
      "average total reward per episode batch since episode  100 :  -45.24560492335605\n",
      "average total reward per episode batch since episode  200 :  -57.9082404780888\n",
      "average total reward per episode batch since episode  300 :  -68.75795970895335\n"
     ]
    }
   ],
   "source": [
    "######################### renders image from third person perspective for validating policy ##############################\n",
    "#env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=True, renders=False, isDiscrete=True, render_mode='tp_camera') \n",
    "##########################################################################################################################\n",
    "\n",
    "######################### renders image from onboard camera ###############################################################\n",
    "# env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=True, renders=False, isDiscrete=True, render_mode='fp_camera') \n",
    "##########################################################################################################################\n",
    "\n",
    "######################### if running locally you can just render the environment in pybullet's GUI #######################\n",
    "#env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=True, renders=False, isDiscrete=True,render_mode='fp_camera') \n",
    "##########################################################################################################################\n",
    "\n",
    "env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=False, isDiscrete = True) \n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "# Neural network params\n",
    "l1 = 4   # this is set as two as we are just inputting x and y , this environment has more available inputs                    \n",
    "l2 = 32                         # hidden layer dimension (we choose this)\n",
    "l3 = env.action_space.n                         # output layer dimension - same as action space\n",
    "alpha = 0.0001                    # learning rate for updating the neural network weights\n",
    "\n",
    "# assemble neural network based on above params \n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(l1, l2),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(l2, l3)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()                                # mean square error loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=alpha)  # optimizer for updating neural network weights\n",
    "\n",
    "\n",
    "gamma = 0.4              # discount factor - determines how much to value future actions\n",
    "episodes = 301           # number of episodes to play out\n",
    "max_episode_length = 400\n",
    "\n",
    "approx_q_learning(env, gamma, episodes, max_episode_length, model, loss_fn, optimizer)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9811888194754665\n",
      "3.2768865893445716\n",
      "2.4640061204741817\n",
      "1.589009623020785\n",
      "0.6793648916151049\n",
      "-0.24860983058794328\n",
      "-0.9875846069721764\n",
      "-1.731221800977117\n",
      "-2.476097496127644\n",
      "-3.220040110446491\n",
      "-3.9616009932366785\n",
      "-4.69975636208003\n",
      "-5.057676832480474\n",
      "-5.414583602965048\n",
      "-5.770871905529714\n",
      "-5.665376994730155\n",
      "-5.421814823613974\n",
      "-5.166858070041346\n",
      "-4.906835996092503\n",
      "-4.644954350969075\n",
      "-4.383298071445009\n",
      "-4.12317866431717\n",
      "-4.020186298733366\n",
      "-3.7930786672084564\n",
      "-3.4845122936290074\n",
      "-3.1239855439556115\n",
      "-2.729509154070061\n",
      "-2.317885108278509\n",
      "-2.0827495969251193\n",
      "-1.842904509525678\n",
      "-1.5931262316280521\n",
      "-1.3325577201890408\n",
      "-1.0643247510441527\n",
      "-0.7851550891256553\n",
      "-0.4961121359431122\n",
      "-0.19929447687518087\n",
      "0.1034929732585248\n",
      "0.41799876361223276\n",
      "0.74593614058097\n",
      "1.089169378684018\n",
      "1.446922508329755\n",
      "1.818962118026405\n",
      "2.2053619284294217\n",
      "2.6063528222850096\n",
      "3.0222459111561424\n",
      "3.453378695829977\n",
      "3.5560113982685913\n",
      "3.010723769483361\n",
      "2.9339814121345413\n",
      "2.9968500884227476\n",
      "3.0686823643019405\n",
      "3.132412741613148\n",
      "3.184657285143058\n",
      "3.223146792710771\n",
      "3.2466529264061634\n",
      "3.254587025055523\n",
      "3.246705995610604\n",
      "3.223045655146232\n",
      "3.1837976409626014\n",
      "3.1292320323874687\n",
      "3.0596927424542604\n",
      "2.9755828816663517\n",
      "2.877354831552672\n",
      "-50\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=True, renders=True, isDiscrete=True,render_mode = 'fp_camera')\n",
    "state, info = env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    q_values = model(torch.tensor([state], dtype=torch.float32))  # predict q-values using learnt model\n",
    "    action = q_values.argmax().item()                             # select action with highest predicted q-value\n",
    "    state, reward, done,_, info = env.step(action)\n",
    "    #time.sleep(1)\n",
    "    print(reward)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
