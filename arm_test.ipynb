{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import simple_driving\n",
    "#import pybullet_envs\n",
    "import pybullet as p\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import random\n",
    "\n",
    "#display = Display(visible=0, size=(400, 300))\n",
    "#display.start()\n",
    "\n",
    "def display_video(frames, framerate=30):\n",
    "  \"\"\"Generates video from `frames`.\n",
    "\n",
    "  Args:\n",
    "    frames (ndarray): Array of shape (n_frames, height, width, 3).\n",
    "    framerate (int): Frame rate in units of Hz.\n",
    "\n",
    "  Returns:\n",
    "    Display object.\n",
    "  \"\"\"\n",
    "  height, width, _ = frames[0].shape\n",
    "  dpi = 70\n",
    "  orig_backend = matplotlib.get_backend()\n",
    "  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "  matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "  ax.set_axis_off()\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_position([0, 0, 1, 1])\n",
    "  im = ax.imshow(frames[0])\n",
    "  def update(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "  interval = 1000/framerate\n",
    "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                  interval=interval, blit=True, repeat=False)\n",
    "  return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(env, state, episodes, episode, model):\n",
    "    \"\"\"Selects an action to take based on a uniformly random sampled number. \n",
    "    If this number is greater than epsilon then returns action with the largest\n",
    "    Q-value at the current state. Otherwise it returns a random action. This\n",
    "    version decays epsilon from a large number to a small number over the duration\n",
    "    of training. This results in highly likely random actions at the start and\n",
    "    eventually biasing actions towards those with high q-values towards the end.\n",
    "\n",
    "    Args:\n",
    "        env: gym object.\n",
    "        state: current state\n",
    "        episodes: maximum number of episodes\n",
    "        episode: number of episodes played so far\n",
    "        model: Q-function approximator\n",
    "\n",
    "    Returns:\n",
    "        Action to be executed for next step.\n",
    "    \"\"\"\n",
    "    EPS_START = 0.99\n",
    "    EPS_END = 0.15\n",
    "    EPS_DECAY = episodes\n",
    "    sample = np.random.uniform(0, 1)\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * episode / EPS_DECAY)\n",
    "    if sample > eps_threshold:\n",
    "        q_values = model(torch.tensor([state], dtype=torch.float32))    # predict q-values for state\n",
    "        return q_values.argmax().item()                                 # return action with highest q-value\n",
    "    else:\n",
    "        return np.random.choice(np.array(range(8)))  # incorporate prior here to prevent flapping too much during exploration (agent will always keep trying to fly into the sky otherwise)\n",
    "        #return np.random.choice([6,7,8])\n",
    "\n",
    "def simulate(env, max_episode_length, episodes, episode, model):\n",
    "    \"\"\"Rolls out an episode of actions to be used for learning.\n",
    "\n",
    "    Args:\n",
    "        env: gym object.\n",
    "        episodes: maximum number of episodes\n",
    "        episode: number of episodes played so far\n",
    "\n",
    "    Returns:\n",
    "        Dataset of episodes for training the RL agent containing states, actions and rewards.\n",
    "    \"\"\"\n",
    "    D = []\n",
    "    state = env.reset()                                                   # line 2\n",
    "    done = False\n",
    "    env.getExtendedObservation()\n",
    "    prev_reward = 0\n",
    "    #while True:                                                             # here I chose to not let episode end but you could replace with the line below\n",
    "    for step in range(max_episode_length):                                  # line 3\n",
    "        action = epsilon_greedy(env, state, episodes, episode, model)  # line 4\n",
    "        next_state, reward, done,info = env.step(action)                   # line 5\n",
    "\n",
    "        D.append([state, action, reward, next_state])                       # line 7\n",
    "        state = next_state                                                  # line 8\n",
    "        if done:                                                            # if we fall into\n",
    "            break\n",
    "    return D                                                                # line 10\n",
    "\n",
    "def approx_q_learning(env, gamma, episodes, max_episode_length, model, loss_fn, optimizer):\n",
    "    \"\"\"Main loop of Approximate Q-learning algorithm.\n",
    "\n",
    "    Args:\n",
    "        env: gym object.\n",
    "        gamma: discount factor - determines how much to value future actions\n",
    "        episodes: number of episodes to play out\n",
    "        max_episode_length: maximum number of steps for episode roll out\n",
    "        model: Q-function approximator\n",
    "        loss_fn: the loss function for our function approximator\n",
    "        optimizer: for backpropagating the gradient of our loss\n",
    "\n",
    "    Returns:\n",
    "        Q-function which is used to derive policy.\n",
    "    \"\"\"\n",
    "    total_reward = 0\n",
    "    for episode in range(episodes):                                             # slightly different to line 3, we just run until maximum episodes played out\n",
    "        D = simulate(env, max_episode_length, episodes, episode, model)    # line 4\n",
    "        q_values_batch = []\n",
    "        target_batch = []\n",
    "        for data in D:                                                          # data = [state, action, reward, next_state]  (line 5)\n",
    "            ####################### update Q values (line 6-7) #########################\n",
    "            state  = data[0]\n",
    "            action = data[1]\n",
    "            reward = data[2]\n",
    "            #print(f\"this is reward = {reward}\")\n",
    "            next_state = data[3]\n",
    "            q_values = model( torch.tensor([state], dtype=torch.float32) )            # predict Q-value for current state\n",
    "            q_values_next = model(torch.tensor([next_state], dtype=torch.float32))  # predict Q-value for next state\n",
    "            target = q_values.clone().detach()\n",
    "            target[0][action] = reward + gamma * q_values_next.max().item()         # loss between prediction and true Q-value we just found from interacting with the env\n",
    "            total_reward += data[2]\n",
    "            loss = loss_fn(q_values, target)                                        # compute loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()                                                         # backpropogate the loss (compute gradients)\n",
    "            optimizer.step()                                                        # update model using gradients\n",
    "            #########################################################################    \n",
    "        if episode % 100 == 0:\n",
    "            print(\"average total reward per episode batch since episode \", episode, \": \", total_reward/ float(100))\n",
    "            total_reward = 0\n",
    "    return model  # line 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average total reward per episode batch since episode  0 :  -1.6799673772650954\n",
      "average total reward per episode batch since episode  100 :  -107.30273432748776\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m episodes \u001b[39m=\u001b[39m \u001b[39m7001\u001b[39m           \u001b[39m# number of episodes to play out\u001b[39;00m\n\u001b[0;32m     38\u001b[0m max_episode_length \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m\n\u001b[1;32m---> 40\u001b[0m approx_q_learning(env, gamma, episodes, max_episode_length, model, loss_fn, optimizer)\n\u001b[0;32m     41\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[31], line 94\u001b[0m, in \u001b[0;36mapprox_q_learning\u001b[1;34m(env, gamma, episodes, max_episode_length, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     92\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     93\u001b[0m     loss\u001b[39m.\u001b[39mbackward()                                                         \u001b[39m# backpropogate the loss (compute gradients)\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()                                                        \u001b[39m# update model using gradients\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[39m#########################################################################    \u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mrham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mrham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\mrham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\mrham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\mrham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    389\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[0;32m    393\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################### renders image from third person perspective for validating policy ##############################\n",
    "#env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=True, renders=False, isDiscrete=True, render_mode='tp_camera') \n",
    "##########################################################################################################################\n",
    "\n",
    "######################### renders image from onboard camera ###############################################################\n",
    "# env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=True, renders=False, isDiscrete=True, render_mode='fp_camera') \n",
    "##########################################################################################################################\n",
    "\n",
    "######################### if running locally you can just render the environment in pybullet's GUI #######################\n",
    "#env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=True, renders=False, isDiscrete=True,render_mode='fp_camera') \n",
    "##########################################################################################################################\n",
    "\n",
    "env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=False, isDiscrete = True) \n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "# Neural network params\n",
    "l1 = 4   # this is set as two as we are just inputting x and y , this environment has more available inputs                    \n",
    "l2 = 32                         # hidden layer dimension (we choose this)\n",
    "l3 = env.action_space.n                         # output layer dimension - same as action space\n",
    "alpha = 0.0001                    # learning rate for updating the neural network weights\n",
    "\n",
    "# assemble neural network based on above params \n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(l1, l2),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(l2, l3)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()                                # mean square error loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=alpha)  # optimizer for updating neural network weights\n",
    "\n",
    "\n",
    "gamma = 0.4              # discount factor - determines how much to value future actions\n",
    "episodes = 7001           # number of episodes to play out\n",
    "max_episode_length = 10000\n",
    "\n",
    "approx_q_learning(env, gamma, episodes, max_episode_length, model, loss_fn, optimizer)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6565967505576269\n",
      "0.6720379062009868\n",
      "0.7457403663612363\n",
      "0.563736263531227\n",
      "0.631991508787218\n",
      "0.44376602484689953\n",
      "0.5104164591871845\n",
      "0.5740854469435681\n",
      "0.6347561015160595\n",
      "0.4492726982656233\n",
      "0.5106666282719814\n",
      "0.5692224238436714\n",
      "0.6247338805224638\n",
      "0.6769904804805791\n",
      "0.7023612085009137\n",
      "0.6302228550336872\n",
      "0.5572017659779056\n",
      "0.4835686304168486\n",
      "0.6315435745106955\n",
      "0.5579379522061332\n",
      "0.6634239308155383\n",
      "0.5478164027653465\n",
      "0.4283558608411139\n",
      "0.35173719130695236\n",
      "0.2731808682370026\n",
      "0.1927909873836322\n",
      "0.11067608816985608\n",
      "0.026948906350910518\n",
      "-0.05827388855339888\n",
      "-0.1448719692285937\n",
      "-0.23272163844283977\n",
      "-0.13132829486102726\n",
      "-0.03849851894686829\n",
      "0.04483796546334684\n",
      "-0.07563406437519804\n",
      "0.003363730134708698\n",
      "-0.11489726771058073\n",
      "-0.040408300895440674\n",
      "-0.1894030287121794\n",
      "-0.34861578388494285\n",
      "-0.5171976987999312\n",
      "-0.5038478361010879\n",
      "-0.6797427992680358\n",
      "-0.666401425285597\n",
      "-0.6710024246843043\n",
      "-0.6859358711202346\n",
      "-0.7070153490983246\n",
      "-0.732329355428615\n",
      "-0.7611592558027953\n",
      "-0.7154554961718245\n",
      "-0.6601911273182115\n",
      "-0.6000431140442304\n",
      "-0.7049135331691043\n",
      "-0.6380179027742927\n",
      "-0.5716261177720238\n",
      "-0.5063803229970048\n",
      "-0.4426865221032221\n",
      "-0.38080980507158846\n",
      "-0.32093017716726746\n",
      "-0.2631752110363186\n",
      "-0.11316973281431841\n",
      "0.02811208885747707\n",
      "0.15973452595121385\n",
      "0.140423926333746\n",
      "0.11682685708951424\n",
      "0.25204433703834583\n",
      "0.23075255651122062\n",
      "0.2048195166802953\n",
      "0.3425311845628036\n",
      "0.31833836317850994\n",
      "0.28955137905236994\n",
      "0.29949000468803877\n",
      "0.30689113491669884\n",
      "0.31182603698804034\n",
      "0.3143718924861135\n",
      "0.31461160554369394\n",
      "0.42656679924346474\n",
      "0.5346699203976135\n",
      "0.6350956337456654\n",
      "0.48806306302406033\n",
      "0.6196080315709473\n",
      "0.5962747196589979\n",
      "0.6041757800637848\n",
      "0.624314942696568\n",
      "0.5360723209164843\n",
      "0.6517879147137119\n",
      "0.5603943166323673\n",
      "0.4612035039984988\n",
      "0.5829602873336892\n",
      "0.48156600804775496\n",
      "0.6048747474324057\n",
      "0.5014941005281813\n",
      "0.6256670394632542\n",
      "0.6297955933720443\n",
      "0.6453360430045129\n",
      "0.6050837893657776\n",
      "0.46493931319404475\n",
      "0.5789238665933089\n",
      "0.4404660416353306\n",
      "0.30085231019948083\n",
      "0.4100488247237419\n",
      "0.25895251405636543\n",
      "0.35504605202395795\n",
      "0.201426830485463\n",
      "0.29050694801769583\n",
      "0.13673403920203742\n",
      "-0.01850748877932018\n",
      "0.06962583987852322\n",
      "-0.08362251017577194\n",
      "-0.23869919845880028\n",
      "-0.39489323109836694\n",
      "-0.5516896071288997\n",
      "-0.7087819972546672\n",
      "-0.8664633295826565\n",
      "-0.9740638370713739\n",
      "-1.0825021576101188\n",
      "-1.0855970083468238\n",
      "-1.089879967303779\n",
      "-1.0952018026078862\n",
      "-1.1016169396507902\n",
      "-1.109737276215437\n",
      "-1.1195318095712186\n",
      "-1.1310425574543737\n",
      "-1.144338343898809\n",
      "-1.1595016095924948\n",
      "-1.1766206716850203\n",
      "-1.19578962106422\n",
      "-1.2170903018823693\n",
      "-1.240608345619823\n",
      "-1.26642482150123\n",
      "-1.2946156286797301\n",
      "-1.3252510698119842\n",
      "-1.3583955314629865\n",
      "-1.3941072378982677\n",
      "-1.4324380545449726\n",
      "-1.4734333369897112\n",
      "-1.4879485420856324\n",
      "-1.4719269302465716\n",
      "-1.4574452198223622\n",
      "-1.4449961227808272\n",
      "-1.434886813842739\n",
      "-1.4273117994243787\n",
      "-1.4223451870764012\n",
      "-1.4200673090036786\n",
      "-1.4204689416496952\n",
      "-1.4235504260393061\n",
      "-1.429304338614898\n",
      "-1.4377304304996896\n",
      "-1.448817999154555\n",
      "-1.4624600712952018\n",
      "-50\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=True, renders=True, isDiscrete=True,render_mode = 'fp_camera')\n",
    "state, info = env.reset()\n",
    "\n",
    "for i in range(10000):\n",
    "    q_values = model(torch.tensor([state], dtype=torch.float32))  # predict q-values using learnt model\n",
    "    action = q_values.argmax().item()                             # select action with highest predicted q-value\n",
    "    state, reward, done,_, info = env.step(action)\n",
    "    time.sleep(0.05)\n",
    "    print(reward)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
